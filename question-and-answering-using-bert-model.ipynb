{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Step 1: Introduction to Question Answering\n\nQuestion Answering (QA) is an NLP task focused on building systems that can automatically answer questions posed by humans in a natural language. For this project, we'll focus on **Extractive Question Answering**.\n\nIn this type of QA, the model is given a **question** and a **context** (a piece of text). The model's job is not to generate a new answer from scratch, but to **find and extract** the span of text from the context that contains the correct answer.\n\nModels based on the **BERT (Bidirectional Encoder Representations from Transformers)** architecture excel at this. Because BERT reads the entire sequence of text at once, it develops a deep, bidirectional understanding of the context, making it highly effective at determining how a question relates to a specific part of the text.\n\n---\n\n# ‚öôÔ∏è Step 2: Setup and Library Installation\n\nThe setup is straightforward. We just need the **transformers** library and a deep learning framework like **PyTorch**, which transformers will use on the backend.\n","metadata":{}},{"cell_type":"code","source":"# Install the necessary libraries\n!pip install -q transformers torch\n\nprint(\"‚úÖ Libraries installed successfully.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T15:12:35.874459Z","iopub.execute_input":"2025-08-13T15:12:35.875253Z","iopub.status.idle":"2025-08-13T15:12:39.072454Z","shell.execute_reply.started":"2025-08-13T15:12:35.875215Z","shell.execute_reply":"2025-08-13T15:12:39.071494Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Libraries installed successfully.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# üöÄ Step 3: Loading the Question Answering Pipeline\n\nHugging Face's `pipeline` function simplifies the process immensely. By specifying the task as **\"question-answering\"**, the library automatically downloads and caches a default pre-trained model fine-tuned for this purpose. The default is often a **DistilBERT** model fine-tuned on the **SQuAD (Stanford Question Answering Dataset)**, which is lightweight yet powerful.\n","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load the question-answering pipeline\n# This will download the default model and tokenizer for the task\nYoutubeer = pipeline(\"question-answering\")\n\nprint(\"Question-Answering pipeline is ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T15:13:41.186668Z","iopub.execute_input":"2025-08-13T15:13:41.187375Z","iopub.status.idle":"2025-08-13T15:13:45.546383Z","shell.execute_reply.started":"2025-08-13T15:13:41.187346Z","shell.execute_reply":"2025-08-13T15:13:45.545773Z"}},"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"05817233c9de4005816d0389a29f54c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"054c5518c99c4396985a8cee9a652595"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9942ae73ecf74da9b8edb0585e80e31b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ce9ae7bb781489bbdc0b9205f297014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f024c89552404d5d9b4eff0aa0837458"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Question-Answering pipeline is ready.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# üìù Step 4: Providing Context and Asking a Question\n\nTo get an answer, we need to provide two key pieces of information to our pipeline:\n\n- **context**: The body of text where the answer can be found.  \n- **question**: The specific question you want to ask about the context.\n\nLet's start with a context about the planet **Mars**.\n","metadata":{}},{"cell_type":"code","source":"# Define the context text\ncontext = \"\"\"\nMars is the fourth planet from the Sun and the second-smallest planet in the Solar System, \nbeing larger than only Mercury. In English, Mars carries the name of the Roman god of war \nand is often referred to as the \"Red Planet\". The latter refers to the effect of the iron\noxide prevalent on Mars's surface, which gives it a reddish appearance distinctive among\nthe astronomical bodies visible to the naked eye.\n\"\"\"\n\n# Define the question\nquestion = \"What gives Mars its reddish appearance?\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T15:14:22.793607Z","iopub.execute_input":"2025-08-13T15:14:22.794282Z","iopub.status.idle":"2025-08-13T15:14:22.797686Z","shell.execute_reply.started":"2025-08-13T15:14:22.794258Z","shell.execute_reply":"2025-08-13T15:14:22.797067Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# üí° Step 5: Getting the Answer from the Model\n\nNow, we simply pass the **question** and **context** to our `question-answering` pipeline.  \nThe model will process both and return a dictionary containing:\n\n- **answer** ‚Äî the exact text span found in the context.  \n- **score** ‚Äî the model's confidence in its answer.  \n- **start** / **end** ‚Äî the character positions of the answer within the context.\n","metadata":{}},{"cell_type":"code","source":"# Get the answer from the pipeline\nresult = Youtubeer(question=question, context=context)\n\n# Print the result in a nice format\nprint(f\"Question: {question}\")\nprint(f\"Answer: '{result['answer']}'\")\nprint(f\"Confidence Score: {result['score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T15:15:40.473231Z","iopub.execute_input":"2025-08-13T15:15:40.473510Z","iopub.status.idle":"2025-08-13T15:15:40.502805Z","shell.execute_reply.started":"2025-08-13T15:15:40.473493Z","shell.execute_reply":"2025-08-13T15:15:40.502262Z"}},"outputs":[{"name":"stdout","text":"Question: What gives Mars its reddish appearance?\nAnswer: 'iron\noxide'\nConfidence Score: 0.8649\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# üß† Step 6: Trying a Different Question\n\nThe same `question-answering` pipeline object can handle **any** question-context pair.  \nLet's ask a new question using the **same** context to see how it performs.  \n\nThis shows that the model is **dynamically** interpreting the text each time,  \nrather than memorizing a single fixed answer.\n","metadata":{}},{"cell_type":"code","source":"# Define a second question\nquestion_2 = \"Which planet is the second smallest in our solar system?\"\n\n# Get the new answer\nresult_2 = Youtubeer(question=question_2, context=context)\n\n# Print the new result\nprint(f\"Question: {question_2}\")\nprint(f\"Answer: '{result_2['answer']}'\")\nprint(f\"Confidence Score: {result_2['score']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T15:16:34.508274Z","iopub.execute_input":"2025-08-13T15:16:34.508532Z","iopub.status.idle":"2025-08-13T15:16:34.521581Z","shell.execute_reply.started":"2025-08-13T15:16:34.508516Z","shell.execute_reply":"2025-08-13T15:16:34.520887Z"}},"outputs":[{"name":"stdout","text":"Question: Which planet is the second smallest in our solar system?\nAnswer: 'Mars'\nConfidence Score: 0.8532\n","output_type":"stream"}],"execution_count":10}]}